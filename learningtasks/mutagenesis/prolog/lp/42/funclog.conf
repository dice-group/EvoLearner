[main]
; configuration options taken from the GILPS manual
; http://www.doc.ic.ac.uk/~jcs06/GILPS/Manual.pdf

; If true and there are output variables in the modeh declaration, stops the
; construction of the most-specific clause in the earliest layer where all
; the output variables have already occurred. If false or there are no output
; variables in the modeh declaration, constructs the bottom clause normally
; up to depth ’i’. This setting is only applicable to ILP engines which use
; most-specific clauses (i.e. ProGolem and FuncLog).
; (Default=false)
;bottom_early_stop=false

; Defines the maximum number of literals (including the head) of a valid
; clause. ProGolem ignores this setting but TopLog and FuncLog adhere to it.
; (Default=4)
;clause_length=4

; Number of folds to perform cross-validation. A value of '1' instructs GILPS
; to use all examples for training. Remember that when an example is
; specified the user can pre-assign it to a specific fold
; (example(bind(p1T 10), −1, 3) --> bound to fold 3) . If there is no
; pre-assigned fold for an example, it will be assigned to a random fold.
; (Default=1)
;cross_validation_folds=1

; If 'true' applies the cut-transformation. The cut-transformation can only
; be applied if 'clause_evaluation=left_to_right'. The purpose of this
; transformation is to speed-up clause evaluation by transforming the clause
; before coverage computation. Although still applicable, the
; cut-transformation has mostly been superseded by the more sophisticated
; clause evaluation engines available through the 'clause_evaluation' setting.
; (Default=false)
;cut_transformation=false

; Defines which function to use when scoring a clause. Suppose this clause
; has NL literals and covers TP true positive examples, FP false positive
; examples, TN true negative examples and FN false negative examples. The
; total number of examples, E, is TP + FP + TN + FN. The most relevant
; scoring functions are:
; - 'accuracy': (TP + TN)/E
; - 'compression': TP − FP − NL
; - 'compression ratio': (TP − FP)/NL
; - 'coverage': TP − FP
; - 'novelty': TP/N − ((TP + FN) ∗ (TP + FP))/(E ∗ E)
; - 'precision': TP/(TP + FP)
; For a full list of available scoring functions see module
; 'hypotheses/score.pl', where you can also specify your own scoring function.
; (Default=compression)
;evalfn=compression

; Maximum depth for the proof of any clause. This setting is important to
; ensure the interpreter does not enter an infinite loop when evaluating
; badly behaved recursive hypotheses or background knowledge.
; (Default=20)
;depth=20

; The weight of each example as specified in the examples definition is
; multiplied by this factor. Remember that when defining the examples, it is
; possible to assign a custom weight for each example, therefore allowing
; some examples to be more important than others. Also notice that if
; 'example_inflation' is a negative number, the positive and negative
; examples swap places. See also positive_example_inflation and
; negative_example_inflation.
; (Default=1)
;example_inflation=1

; Defines the number of layers of new variables when constructing the
; most-specific clause for an example. This setting is ignored by TopLog
; as it does not need to construct most-specific clauses.
; (Default=3)
;i=3

; Maximum weight of negative examples that may be covered by a single clause.
; (Default=inf)
;maxneg=inf

; Maximum number of hypotheses in the final induced theory. The default value
; of 'inf' allows the addition of whatever number of clauses may be needed,
; as long as there is an incremental gain in adding these clauses to the
; current theory. The incremental gain is measured according to the evalfn
; setting and also considers the positive and negative examples the theory
; covers so far.
; (Default=inf)
;max_clauses_per_theory=inf

; Maximum number of uncompressive examples (or negative score if other
; scoring function is being used) allowed before stopping the search and
; computing the current best theory. This setting is only applicable if
; theory_construction=incremental.
; (Default=20)
;max_uncompressive_examples=20

; Minimum percentage accuracy a clause has to have on the training data to
; be considered a valid hypothesis.
; (Default=0)
;minacc=0

; Minimum percentage of the positive examples a clause has to cover to be
; considered a valid hypothesis.
; (Default=0)
;mincov=0

; Minimum weight of positive examples a clause has to cover to be considered
; a valid hypothesis.
; (Default=0)
;minpos=0

; Minimum percentage of corrected predicted positive examples a clause has to
; have to be considered a valid hypothesis.
; (Default=0)
;minprec=0

; Multiplies the weights of all negative examples by this factor. See also
; example_inflation and positive_example_inflation.
; (Default=1)
;negative_example_inflation=1

; Maximum percentage of negative weights a clause may cover to still be
; considered a valid hypothesis.
; (Default=0.5)
;noise=0.5

; Multiplies the weights of all positive examples by this factor. See also
; example_inflation and negative_example_inflation.
; (Default=1)
;positive_example_inflation=1

; This setting controls the pretty printing of clauses to the stdout. It
; specifies the number of literals to be displayed per line when showing a
; clause.
; (Default=4)
;print=4

; This is an integer specifying the random seed to be used by the ILP system.
; If the seed is the same across runs, the same numbers will be generated by
; the random number generators and results can be reproduced.
; (Default=7)
;random_seed=7

; This setting impacts the construction of the most-specific clause. For a
; given mode body declaration,modeb, if 'false' the atoms that will be added
; to the body of the most-specific clause are the first N matches of the
; modeb declaration against the background knowledge.
; If randomize_recall=true the N solutions are randomly selected from the set
; of all possible matches of the modeb declaration. This may be useful when
; the dataset has a degree of non-determinism higher than the star default
; recall and we do not want to introduce a bias to favour the first matches.
; In ILP systems which do not have this setting, e.g. Aleph and Progol, it
; is possible to emulate it by shuffling the background knowledge file.
; (Default=false)
;randomize_recall=false

; This is an experimental setting. When evaluating a literal in a clause,
; only the first recall bound on evaluation solutions for any literal are
; considered. The default value of 'inf' considers all the solutions, as is
; expected from Prolog semantics. A lower value would only consider the first
; solutions, which could wrongly conclude that a given clause does not cover
; an example when it does. Note that this setting is unrelated to
; star_default_recall.
; (Default=inf)
;recall_bound_on_evaluation=inf

; This setting is only applicable when theory_construction=incremental.
; If set to true, when asserting a new hypothesis to the theory, and in
; addition to remove the positive examples covered by this hypothesis clause,
; the negative examples the hypothesis covers are also removed.
; (Default=false)
;remove_negatives=false

; This is a real number between 0.0 and 1.0 specifying the approximate
; percentage of the user-supplied examples (both positive and negative) to be
; used by the ILP system. In order to speed-up the learning in datasets where
; there are too many examples, it may be useful to use a small fraction of
; the total examples. Ideally the ILP system should already do some form of
; sample coverage and that is planned for a future version of GILPS.
; (Default=1.0)
;sample=1.0

; The integer value specifying the recall to which a '*' ought to correspond
; in a modeb definition. The recall setting is an important setting that is
; used in the construction of the most-specific clause of an example. A
; higher recall implies a larger hypothesis space.
; (Default=10)
;star_default_recall=10

; In 'global' construction mode the theory is constructed after all
; hypotheses have been generated. The other possibility is 'incremental'
; construction. In 'incremental' mode there is a loop where the best
; hypothesis generated from an example is asserted to the final theory and
; all the positive examples this hypothesis covers are retracted. Incremental
; theory construction therefore requires fewer CPU resources than global
; coverage but is example order dependent and may lead to weaker theories.
; (Default=global)
;theory_construction=global

; An integer >= 0 controlling the verbosity of GILPS. The higher the verbose
; level, the more information is shown.
; (Default=1)
;verbose=1

